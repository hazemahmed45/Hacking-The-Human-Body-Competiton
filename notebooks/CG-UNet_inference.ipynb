{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        #print(os.path.join(dirname, filename))\n        pass\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-23T07:43:28.813172Z","iopub.execute_input":"2022-12-23T07:43:28.813642Z","iopub.status.idle":"2022-12-23T07:43:29.153004Z","shell.execute_reply.started":"2022-12-23T07:43:28.813544Z","shell.execute_reply":"2022-12-23T07:43:29.151852Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip3 install albumentations","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:43:29.155288Z","iopub.execute_input":"2022-12-23T07:43:29.155671Z","iopub.status.idle":"2022-12-23T07:44:01.765681Z","shell.execute_reply.started":"2022-12-23T07:43:29.155628Z","shell.execute_reply":"2022-12-23T07:44:01.764581Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: albumentations in /opt/conda/lib/python3.7/site-packages (1.3.0)\nRequirement already satisfied: numpy>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.21.6)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from albumentations) (1.7.3)\nRequirement already satisfied: qudida>=0.0.4 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.0.4)\nRequirement already satisfied: opencv-python-headless>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (4.5.4.60)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from albumentations) (6.0)\nRequirement already satisfied: scikit-image>=0.16.1 in /opt/conda/lib/python3.7/site-packages (from albumentations) (0.19.3)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.4.0)\nRequirement already satisfied: scikit-learn>=0.19.1 in /opt/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\nRequirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.1.1)\nRequirement already satisfied: networkx>=2.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.5)\nRequirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.3.0)\nRequirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\nRequirement already satisfied: imageio>=2.4.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.19.3)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.2->scikit-image>=0.16.1->albumentations) (5.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.9)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.0.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from albumentations.core.transforms_interface import ImageOnlyTransform\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import Compose,Resize\n\nclass Normalization(ImageOnlyTransform):\n    \"\"\"\n    this class normalize the input image by dividing image values by 255\n\n    \"\"\"\n    def __init__(self, always_apply: bool = True, p: float = 1):\n        super().__init__(always_apply, p)\n    \n    def apply(self, img, **params) :\n        return img/255.0\ntransform=Compose([\n        Resize(height=224,width=224),\n        Normalization(always_apply=True),\n        ToTensorV2()])","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:44:01.767228Z","iopub.execute_input":"2022-12-23T07:44:01.767711Z","iopub.status.idle":"2022-12-23T07:44:05.361718Z","shell.execute_reply.started":"2022-12-23T07:44:01.767661Z","shell.execute_reply":"2022-12-23T07:44:05.360526Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom tqdm import tqdm\nimport cv2\n\ndef listImagesFilesInDir(dir_path:str)->list:\n    files_list = [filename for filename in os.listdir(dir_path) if\n                  (str.lower(filename[-4:]) in ['.jpg', '.png']) or (str.lower(filename[-5:]) in ['.jpeg','.tiff'])]\n    return files_list\n\nclass CropSegDatasetInfer(Dataset):\n    \"\"\"\n    dataloader that read dataset in the directory tree form only\n    ---dataset\n            |\n            |----img\n\n    \"\"\"\n    def __init__(self, dir_path,split, transforms=None):\n        super(CropSegDatasetInfer, self).__init__()\n        \"\"\"\n        dataset constructor\n\n        attributes:-\n\n        - dir_path : split path inside the dataset path\n        - transforms : augmentation or transformation needs to be done on the image\n        - data : list of paths that will be loaded during training item by item\n\n        :param dir_path: dataset path\n        :type dir_path: str\n        :param split: split name\n        :type split: str\n        :param transform: transformations to be applied on each image\n        :type transform: albumentations.Compose\n        \"\"\"\n        self.dir_path = os.path.join(dir_path,split)\n        self.transforms=transforms\n        self.data = self.get_data_list()\n    def get_data_list(self) :\n        input_and_labels = []\n        images_dir = os.path.join(self.dir_path)\n        # labels_dir = os.path.join(self.dir_path, DictKeys.LBL.value)\n        \n        assert os.path.exists(images_dir), f'image directory @ {images_dir} does not exist'\n        # assert os.path.exists(labels_dir), f'image directory @ {labels_dir} does not exist'\n        images_names = listImagesFilesInDir(images_dir) #list of dataset images names\n        \n        print(f\"[INFO] loading files names from {images_dir}\")\n        for file_name in tqdm(images_names):\n            img_path = os.path.join(images_dir, file_name)\n            # lbl_path = os.path.join(labels_dir, file_name)\n            input_and_labels.append([file_name, img_path])\n\n        return input_and_labels\n    def __getitem__(self, index):\n        \"\"\"_summary_\n\n        :param index: index of the item to be retrieved\n        :type index: int\n        :return: filename of the image read, image in float tensor\n        :rtype: str,torch.FloatTensor\n        \"\"\"\n        file_name, img_path = self.data[index]\n        image = cv2.imread(img_path)#Image.open(img_path).convert('RGB')\n        orig_size=image.shape\n        if(self.transforms is not None):\n            aug_out = self.transforms(image=image)\n            image=aug_out['image']\n\n        return file_name, image,orig_size\n    def __len__(self):\n        return len(self.data) \ndataset=CropSegDatasetInfer('/kaggle/input/hubmap-organ-segmentation/test_images','',transform)\nprint(dataset[0][2])","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:44:05.363774Z","iopub.execute_input":"2022-12-23T07:44:05.364610Z","iopub.status.idle":"2022-12-23T07:44:05.690661Z","shell.execute_reply.started":"2022-12-23T07:44:05.364561Z","shell.execute_reply":"2022-12-23T07:44:05.689540Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[INFO] loading files names from /kaggle/input/hubmap-organ-segmentation/test_images/\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00, 4084.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"(2023, 2023, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nclass double_conv(nn.Module):\n    '''(conv => BN => ReLU) * 2'''\n    def __init__(self, in_ch, out_ch, bn):\n        super(double_conv, self).__init__()\n        if bn:\n            self.conv = nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            self.conv = nn.Sequential(\n            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n            nn.ReLU(inplace=True)\n        )\n\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\nclass inconv(nn.Module):\n    def __init__(self, in_ch, out_ch, bn):\n        super(inconv, self).__init__()\n\n        self.conv = double_conv(in_ch, out_ch, bn)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\nclass down(nn.Module):\n    def __init__(self, in_ch, out_ch, bn):\n        super(down, self).__init__()\n        self.mpconv = nn.Sequential(\n            nn.MaxPool2d(2),\n            double_conv(in_ch, out_ch, bn)\n        )\n\n    def forward(self, x):\n        x = self.mpconv(x)\n        return x\n\nclass up(nn.Module):\n    def __init__(self, in_ch, out_ch, bn, bilinear=True):\n        super(up, self).__init__()\n\n        #  would be a nice idea if the upsampling could be learned too,\n        #  but my machine do not have enough memory to handle all those weights\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode='nearest')\n        else:\n            self.up = nn.ConvTranspose2d(in_ch//2, in_ch//2, 2, stride=2)\n\n        self.conv = double_conv(in_ch, out_ch, bn)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        \n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, (diffX // 2, diffX - diffX//2,\n                        diffY // 2, diffY - diffY//2))\n        \n        # for padding issues, see \n        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n\n        x = torch.cat([x2, x1], dim=1)\n        x = self.conv(x)\n        return x\nclass outconv(nn.Module):\n    def __init__(self, in_ch, out_ch):\n        super(outconv, self).__init__()\n        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\nclass UNetOutMaskClass(torch.nn.Module):\n    \"\"\"\n    Vanilla unet model architecture\n    \"\"\"\n    def __init__(self, n_classes,n_organs, batchnorm=False):\n        super(UNetOutMaskClass, self).__init__()\n        self.inc = inconv(3, 64, batchnorm)\n        self.down1 = down(64, 128, batchnorm)\n        self.down2 = down(128, 256, batchnorm)\n        self.down3 = down(256, 512, batchnorm)\n        self.down4 = down(512, 512, batchnorm)\n        self.up1 = up(1024, 256, batchnorm)\n        self.up2 = up(512, 128, batchnorm)\n        self.up3 = up(256, 64, batchnorm)\n        self.up4 = up(128, 64, batchnorm)\n        self.outc = outconv(64, n_classes)\n        self.pool=torch.nn.AdaptiveAvgPool2d(1)\n        self.fc1=torch.nn.Linear(64,64)\n        self.fc2=torch.nn.Linear(64,64)\n        self.outorgan=torch.nn.Linear(64,n_organs)\n\n    def forward(self,x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        pooled=self.pool(x)\n        pooled=torch.flatten(pooled,start_dim=1)\n        # print(x.shape)\n        x = self.outc(x)\n        # print(pooled.shape)\n        pooled=self.fc2(self.fc1(pooled))\n        \n        organ=self.outorgan(pooled)\n        return x,organ\nclass UNet(torch.nn.Module):\n    \"\"\"\n    Vanilla unet model architecture\n    \"\"\"\n    def __init__(self, n_classes, batchnorm=False):\n        super(UNet, self).__init__()\n        self.inc = inconv(3, 64, batchnorm)\n        self.down1 = down(64, 128, batchnorm)\n        self.down2 = down(128, 256, batchnorm)\n        self.down3 = down(256, 512, batchnorm)\n        self.down4 = down(512, 512, batchnorm)\n        self.up1 = up(1024, 256, batchnorm)\n        self.up2 = up(512, 128, batchnorm)\n        self.up3 = up(256, 64, batchnorm)\n        self.up4 = up(128, 64, batchnorm)\n        self.outc = outconv(64, n_classes)\n#         self.pool=torch.nn.AdaptiveAvgPool2d(1)\n#         self.fc1=torch.nn.Linear(64,64)\n#         self.fc2=torch.nn.Linear(64,64)\n#         self.outorgan=torch.nn.Linear(64,n_organs)\n\n    def forward(self,x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n#         pooled=self.pool(x)\n#         pooled=torch.flatten(pooled,start_dim=1)\n        # print(x.shape)\n        x = self.outc(x)\n        # print(pooled.shape)\n#         pooled=self.fc2(self.fc1(pooled))\n        \n#         organ=self.outorgan(pooled)\n        return x\nclass UNetLight(torch.nn.Module):\n    \"\"\"\n    Vanilla unet model architecture but with less parameters\n    \"\"\"\n    def __init__(self, n_classes, batchnorm=False):\n        super(UNetLight, self).__init__()\n        self.inc = inconv(3, 8, batchnorm)\n        self.down1 = down(8, 16, batchnorm)\n        self.down2 = down(16, 32, batchnorm)\n        self.down3 = down(32, 64, batchnorm)\n        self.down4 = down(64, 64, batchnorm)\n        self.up1 = up(128, 32, batchnorm)\n        self.up2 = up(64, 16, batchnorm)\n        self.up3 = up(32, 8, batchnorm)\n        self.up4 = up(16, 8, batchnorm)\n        self.outc = outconv(8, n_classes)\n\n    def forward(self,x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x = self.outc(x)\n        return x\nmodel=UNetOutMaskClass(2,5,True)#.to('cuda')\nmodel.load_state_dict(torch.load('/kaggle/input/models-hac/HAC-81-UNET_FULL_WITH_CLASS-best_weights.ckpt',map_location='cpu')['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:44:30.451400Z","iopub.execute_input":"2022-12-23T07:44:30.451842Z","iopub.status.idle":"2022-12-23T07:44:31.494765Z","shell.execute_reply.started":"2022-12-23T07:44:30.451805Z","shell.execute_reply":"2022-12-23T07:44:31.493604Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"model","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:44:31.951471Z","iopub.execute_input":"2022-12-23T07:44:31.952380Z","iopub.status.idle":"2022-12-23T07:44:31.959877Z","shell.execute_reply.started":"2022-12-23T07:44:31.952342Z","shell.execute_reply":"2022-12-23T07:44:31.958973Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"UNetOutMaskClass(\n  (inc): inconv(\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (down1): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down2): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down3): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (down4): down(\n    (mpconv): Sequential(\n      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n      (1): double_conv(\n        (conv): Sequential(\n          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (2): ReLU(inplace=True)\n          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n          (5): ReLU(inplace=True)\n        )\n      )\n    )\n  )\n  (up1): up(\n    (up): Upsample(scale_factor=2.0, mode=nearest)\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up2): up(\n    (up): Upsample(scale_factor=2.0, mode=nearest)\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up3): up(\n    (up): Upsample(scale_factor=2.0, mode=nearest)\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (up4): up(\n    (up): Upsample(scale_factor=2.0, mode=nearest)\n    (conv): double_conv(\n      (conv): Sequential(\n        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): ReLU(inplace=True)\n        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (5): ReLU(inplace=True)\n      )\n    )\n  )\n  (outc): outconv(\n    (conv): Conv2d(64, 2, kernel_size=(1, 1), stride=(1, 1))\n  )\n  (pool): AdaptiveAvgPool2d(output_size=1)\n  (fc1): Linear(in_features=64, out_features=64, bias=True)\n  (fc2): Linear(in_features=64, out_features=64, bias=True)\n  (outorgan): Linear(in_features=64, out_features=5, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \nimport numpy as np\nmodel.eval()\ndef vis_mask(mask):\n    import matplotlib.pyplot as plt\n    plt.figure(figsize=(15, 15))\n    plt.imshow(mask)\n    plt.axis(\"off\")\n    plt.show()\n    return \nid_rle={'id':[],'rle':[]}\nwith torch.no_grad():\n    for ii,(file_name,image,original_size) in tqdm(enumerate(dataset),total=len(dataset)):\n        image=image.unsqueeze(dim=0).float()#.to('cuda')\n        mask,_=model(image)\n        mask=torch.argmax(mask,dim=1)\n        mask=np.uint8(mask.detach().squeeze(dim=0).numpy())*255\n        print(mask.shape)\n        mask=cv2.resize(mask,dsize=original_size[:-1],interpolation=cv2.INTER_NEAREST)\n#         vis_mask(mask)\n        id_rle['id'].append(file_name.split('.')[0])\n        id_rle['rle'].append(mask2rle(mask))\npd.DataFrame(id_rle).to_csv('submission.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2022-12-23T07:44:32.850582Z","iopub.execute_input":"2022-12-23T07:44:32.851725Z","iopub.status.idle":"2022-12-23T07:44:33.503713Z","shell.execute_reply.started":"2022-12-23T07:44:32.851681Z","shell.execute_reply":"2022-12-23T07:44:33.502577Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"100%|██████████| 1/1 [00:00<00:00,  1.59it/s]","output_type":"stream"},{"name":"stdout","text":"(224, 224)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}